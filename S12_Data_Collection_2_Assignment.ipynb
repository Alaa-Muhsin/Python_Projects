{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GITHUB_Link https://github.com/Alaa-Muhsin/Python_Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import xlsxwriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1\n",
    "---------\n",
    "Scrap the USD To EGP Exchange rate from this website\n",
    "https://www.xe.com/currencyconverter/convert/?Amount=1&From=USD&To=EGP\n",
    "and then use it to make a software that takes amount of USD Dollars from the user and calculate how much will it cost in EGP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.83"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "res = requests.get(\"https://www.xe.com/currencyconverter/convert/?Amount=1&From=USD&To=EGP\")\n",
    "soup  = BeautifulSoup(res.text,\"html.parser\")\n",
    "egy_pound_tag = soup.find(\"p\",attrs={\"class\":\"sc-b5d1d4ba-1 bPeOTN\"})\n",
    "egy_pound_tag.text\n",
    "egy_pound = egy_pound_tag.text[:5]\n",
    "egy_pound_float = float(egy_pound)\n",
    "egy_pound_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0 $ = 5083.0 EGP Pound\n",
      "B is Invalid input\n",
      "10166.0 EGP Pound = 200.0 $\n",
      "Invalid input\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "        user_Choice = input(\"Hello \\n Please Choose : 1- Converting Currency From Dollar To EGP Pound . \\n2- Converting Currency From EGP Pound To Dollar. \\n3- Exit\")\n",
    "        #Converting From Dollar To EGP Pound\n",
    "        if user_Choice == \"1\" :\n",
    "            try:\n",
    "                dollar = float(input(\"Please Enter Dollar amounts\"))\n",
    "                egy_pound_result = egy_pound_float * dollar\n",
    "                print(f\"{dollar} $ = {egy_pound_result} EGP Pound\")\n",
    "                \n",
    "            except ValueError:\n",
    "                print(\"Invalid input\")\n",
    "                \n",
    "        #Converting From EGP Pound To Dollar\n",
    "        elif user_Choice == \"2\" :\n",
    "            try:\n",
    "                egy_pound = float(input(\"Please Enter EGP Pounds amounts\"))\n",
    "                dollar_result = egy_pound / egy_pound_float\n",
    "                print(f\"{egy_pound} EGP Pound = {dollar_result} $\")\n",
    "                \n",
    "            except ValueError:\n",
    "                print(f\"{user_Choice} is Invalid input\")\n",
    "                \n",
    "        elif user_Choice == \"3\":\n",
    "             break\n",
    "        \n",
    "        else:\n",
    "             print(f\"{user_Choice} is Invalid input\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2\n",
    "--------- \n",
    "Scraping from forecast weather\n",
    "Imagine you work for forecast weather Now imagine your boss wants you to find the temperture for each day in celsius degree. How could you do this with Beautiful Soup?\n",
    "https://forecast.weather.gov/MapClick.php?lat=37.7772&lon=-122.4168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(\"https://forecast.weather.gov/MapClick.php?lat=37.7772&lon=-122.4168\")\n",
    "soup  = BeautifulSoup(res.text,\"html.parser\")\n",
    "with open(\"forecasting_weather.csv\", mode='w', encoding='utf-8') as fd:\n",
    "    #Header\n",
    "    fd.write(\"Period,Temperature_C,Temperature_F,Description\\n\")\n",
    "\n",
    "    #Current Weather\n",
    "    current_temperature_C = soup.find(\"p\",attrs={\"class\":\"myforecast-current-sm\"}).text[:2]\n",
    "    current_temperature_F = soup.find(\"p\",attrs={\"class\":\"myforecast-current-lrg\"}).text[:2]\n",
    "    forecast_tags_current_weather = soup.find(\"div\",attrs={\"class\":\"tombstone-container\"}).find_all(\"p\")\n",
    "    current_period = forecast_tags_current_weather[0].text.strip(\"\\n \")\n",
    "    current_description = forecast_tags_current_weather[2].text\n",
    "\n",
    "    #First_Row = Current Weather \n",
    "    fd.write(f\"\\\"{current_period}\\\",\\\"{current_temperature_C}\\\"°C,\\\"{current_temperature_F}\\\"°F,\\\"{current_description}\\\"\\n\")\n",
    "\n",
    "    forecast_tags = soup.find_all(\"div\",attrs={\"class\":\"tombstone-container\"})\n",
    "    for i in range(1,len(forecast_tags)):\n",
    "        forecast_imformation = forecast_tags[i].find_all(\"p\")\n",
    "        period_name = forecast_imformation[0].text\n",
    "        temperature = forecast_imformation[2].text.split(\" \")\n",
    "        temperature_in_F = float(temperature[1])\n",
    "        temperature_in_C = round((temperature_in_F - 32) * 5/9 ,2)\n",
    "\n",
    "        Description = forecast_imformation[3].text\n",
    "        fd.write(f\"\\\"{period_name}\\\",\\\"{temperature_in_C}\\\"°C,\\\"{temperature_in_F}\\\"°F,\\\"{Description}\\\"\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Period</th>\n",
       "      <th>Temperature_C</th>\n",
       "      <th>Temperature_F</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New Year's Day</td>\n",
       "      <td>12°C</td>\n",
       "      <td>53°F</td>\n",
       "      <td>High: 58 °F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tonight</td>\n",
       "      <td>8.89°C</td>\n",
       "      <td>48.0°F</td>\n",
       "      <td>Partly Cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thursday</td>\n",
       "      <td>16.67°C</td>\n",
       "      <td>62.0°F</td>\n",
       "      <td>Mostly Sunny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thursday Night</td>\n",
       "      <td>10.0°C</td>\n",
       "      <td>50.0°F</td>\n",
       "      <td>Partly Cloudythen ChanceRain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Friday</td>\n",
       "      <td>15.56°C</td>\n",
       "      <td>60.0°F</td>\n",
       "      <td>Rain Likely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Friday Night</td>\n",
       "      <td>8.89°C</td>\n",
       "      <td>48.0°F</td>\n",
       "      <td>Chance Rainthen SlightChance Rain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Saturday</td>\n",
       "      <td>14.44°C</td>\n",
       "      <td>58.0°F</td>\n",
       "      <td>Mostly Sunny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Saturday Night</td>\n",
       "      <td>8.33°C</td>\n",
       "      <td>47.0°F</td>\n",
       "      <td>Mostly Cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sunday</td>\n",
       "      <td>15.56°C</td>\n",
       "      <td>60.0°F</td>\n",
       "      <td>Mostly Sunny</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Period Temperature_C Temperature_F  \\\n",
       "0  New Year's Day          12°C          53°F   \n",
       "1         Tonight        8.89°C        48.0°F   \n",
       "2        Thursday       16.67°C        62.0°F   \n",
       "3  Thursday Night        10.0°C        50.0°F   \n",
       "4          Friday       15.56°C        60.0°F   \n",
       "5    Friday Night        8.89°C        48.0°F   \n",
       "6        Saturday       14.44°C        58.0°F   \n",
       "7  Saturday Night        8.33°C        47.0°F   \n",
       "8          Sunday       15.56°C        60.0°F   \n",
       "\n",
       "                         Description  \n",
       "0                        High: 58 °F  \n",
       "1                      Partly Cloudy  \n",
       "2                       Mostly Sunny  \n",
       "3       Partly Cloudythen ChanceRain  \n",
       "4                        Rain Likely  \n",
       "5  Chance Rainthen SlightChance Rain  \n",
       "6                       Mostly Sunny  \n",
       "7                      Mostly Cloudy  \n",
       "8                       Mostly Sunny  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"forecasting_weather.csv\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3\n",
    "---------\n",
    "Scrap the books (name, price, rate) for each category and put them into a CSV & Excel file\n",
    "https://books.toscrape.com/ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Without Category - Without Requesting each book details link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "page_url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
    "res_books = requests.get(page_url)\n",
    "soup_book  = BeautifulSoup(res_books.text,\"html.parser\")\n",
    "\n",
    "#number of pages \n",
    "page_number = int(soup_book.find(\"li\",attrs={\"class\":\"current\"}).text.strip(\"\\n \").split(\"of \")[1])\n",
    "\n",
    "\n",
    "\n",
    "with open(\"Books.csv\", mode='w', encoding='utf-8') as fd:\n",
    "  fd.write(\"Name_in_books_page,Price,Rating,Availability,Link,Image\\n\") # header\n",
    "\n",
    "  for p in range(page_number):\n",
    "    page_url = \"https://books.toscrape.com/catalogue/page-\"+ str(p) + \".html\"\n",
    "\n",
    "    res_books = requests.get(page_url)\n",
    "    soup_book  = BeautifulSoup(res_books.text,\"html.parser\")\n",
    "\n",
    "    book_tag = soup_book.find_all(\"li\",attrs={\"class\":\"col-xs-6 col-sm-4 col-md-3 col-lg-3\"})\n",
    "    length_tag =  len(book_tag)\n",
    "\n",
    "    for i in range(len(book_tag)):\n",
    "\n",
    "      #Book Link\n",
    "      book_details_link = \"https://books.toscrape.com/catalogue/\" + book_tag[i].find(\"div\").find(\"a\").attrs['href']\n",
    "\n",
    "      #book image \n",
    "      book_image = book_tag[i].find(\"img\").attrs[\"src\"]\n",
    "      book_image = \"https://books.toscrape.com/\" + book_image[2:]\n",
    "\n",
    "      #book name\n",
    "      book_name = book_tag[i].find(\"h3\").find(\"a\").text\n",
    "\n",
    "      #book Price\n",
    "      book_price = book_tag[i].find(\"p\",attrs={\"class\":\"price_color\"}).text[2:]\n",
    "\n",
    "      #book Availability \n",
    "      book_availability = book_tag[i].find(\"p\",attrs={\"class\":\"instock availability\"}).text.strip(\"\\n \")\n",
    "\n",
    "      book_rating = book_tag[i].find(\"p\").attrs[\"class\"][1]\n",
    "\n",
    "\n",
    "      fd.write(f\"\\\"{book_name}\\\",\\\"{book_price}\\\",\\\"{book_rating}\\\",\\\"{book_availability}\\\",\\\"{book_details_link}\\\",\\\"{book_image}\\\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Books.csv\")\n",
    " \n",
    "# saving xlsx file\n",
    "df_xlsx = pd.ExcelWriter('Books.xlsx')\n",
    "df.to_excel(df_xlsx, index=False)\n",
    "df_xlsx.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name_in_books_page</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Availability</th>\n",
       "      <th>Link</th>\n",
       "      <th>Image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the ...</td>\n",
       "      <td>51.77</td>\n",
       "      <td>Three</td>\n",
       "      <td>In stock</td>\n",
       "      <td>https://books.toscrape.com/catalogue/a-light-i...</td>\n",
       "      <td>https://books.toscrape.com//media/cache/2c/da/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>53.74</td>\n",
       "      <td>One</td>\n",
       "      <td>In stock</td>\n",
       "      <td>https://books.toscrape.com/catalogue/tipping-t...</td>\n",
       "      <td>https://books.toscrape.com//media/cache/26/0c/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>50.10</td>\n",
       "      <td>One</td>\n",
       "      <td>In stock</td>\n",
       "      <td>https://books.toscrape.com/catalogue/soumissio...</td>\n",
       "      <td>https://books.toscrape.com//media/cache/3e/ef/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>47.82</td>\n",
       "      <td>Four</td>\n",
       "      <td>In stock</td>\n",
       "      <td>https://books.toscrape.com/catalogue/sharp-obj...</td>\n",
       "      <td>https://books.toscrape.com//media/cache/32/51/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History ...</td>\n",
       "      <td>54.23</td>\n",
       "      <td>Five</td>\n",
       "      <td>In stock</td>\n",
       "      <td>https://books.toscrape.com/catalogue/sapiens-a...</td>\n",
       "      <td>https://books.toscrape.com//media/cache/be/a5/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>Icing (Aces Hockey #2)</td>\n",
       "      <td>40.44</td>\n",
       "      <td>Four</td>\n",
       "      <td>In stock</td>\n",
       "      <td>https://books.toscrape.com/catalogue/icing-ace...</td>\n",
       "      <td>https://books.toscrape.com//media/cache/8d/1e/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>Hawkeye, Vol. 1: My ...</td>\n",
       "      <td>45.24</td>\n",
       "      <td>Three</td>\n",
       "      <td>In stock</td>\n",
       "      <td>https://books.toscrape.com/catalogue/hawkeye-v...</td>\n",
       "      <td>https://books.toscrape.com//media/cache/16/46/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>Having the Barbarian's Baby ...</td>\n",
       "      <td>34.96</td>\n",
       "      <td>Four</td>\n",
       "      <td>In stock</td>\n",
       "      <td>https://books.toscrape.com/catalogue/having-th...</td>\n",
       "      <td>https://books.toscrape.com//media/cache/f4/83/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>Giant Days, Vol. 1 ...</td>\n",
       "      <td>56.76</td>\n",
       "      <td>Four</td>\n",
       "      <td>In stock</td>\n",
       "      <td>https://books.toscrape.com/catalogue/giant-day...</td>\n",
       "      <td>https://books.toscrape.com//media/cache/35/0b/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>Fruits Basket, Vol. 1 ...</td>\n",
       "      <td>40.28</td>\n",
       "      <td>Five</td>\n",
       "      <td>In stock</td>\n",
       "      <td>https://books.toscrape.com/catalogue/fruits-ba...</td>\n",
       "      <td>https://books.toscrape.com//media/cache/7c/c1/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>980 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Name_in_books_page  Price Rating Availability  \\\n",
       "0                 A Light in the ...  51.77  Three     In stock   \n",
       "1                 Tipping the Velvet  53.74    One     In stock   \n",
       "2                         Soumission  50.10    One     In stock   \n",
       "3                      Sharp Objects  47.82   Four     In stock   \n",
       "4       Sapiens: A Brief History ...  54.23   Five     In stock   \n",
       "..                               ...    ...    ...          ...   \n",
       "975           Icing (Aces Hockey #2)  40.44   Four     In stock   \n",
       "976          Hawkeye, Vol. 1: My ...  45.24  Three     In stock   \n",
       "977  Having the Barbarian's Baby ...  34.96   Four     In stock   \n",
       "978           Giant Days, Vol. 1 ...  56.76   Four     In stock   \n",
       "979        Fruits Basket, Vol. 1 ...  40.28   Five     In stock   \n",
       "\n",
       "                                                  Link  \\\n",
       "0    https://books.toscrape.com/catalogue/a-light-i...   \n",
       "1    https://books.toscrape.com/catalogue/tipping-t...   \n",
       "2    https://books.toscrape.com/catalogue/soumissio...   \n",
       "3    https://books.toscrape.com/catalogue/sharp-obj...   \n",
       "4    https://books.toscrape.com/catalogue/sapiens-a...   \n",
       "..                                                 ...   \n",
       "975  https://books.toscrape.com/catalogue/icing-ace...   \n",
       "976  https://books.toscrape.com/catalogue/hawkeye-v...   \n",
       "977  https://books.toscrape.com/catalogue/having-th...   \n",
       "978  https://books.toscrape.com/catalogue/giant-day...   \n",
       "979  https://books.toscrape.com/catalogue/fruits-ba...   \n",
       "\n",
       "                                                 Image  \n",
       "0    https://books.toscrape.com//media/cache/2c/da/...  \n",
       "1    https://books.toscrape.com//media/cache/26/0c/...  \n",
       "2    https://books.toscrape.com//media/cache/3e/ef/...  \n",
       "3    https://books.toscrape.com//media/cache/32/51/...  \n",
       "4    https://books.toscrape.com//media/cache/be/a5/...  \n",
       "..                                                 ...  \n",
       "975  https://books.toscrape.com//media/cache/8d/1e/...  \n",
       "976  https://books.toscrape.com//media/cache/16/46/...  \n",
       "977  https://books.toscrape.com//media/cache/f4/83/...  \n",
       "978  https://books.toscrape.com//media/cache/35/0b/...  \n",
       "979  https://books.toscrape.com//media/cache/7c/c1/...  \n",
       "\n",
       "[980 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Books.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Category recommend NOT to run as it takes 10 min estimated time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "page_url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
    "res_books = requests.get(page_url)\n",
    "soup_book  = BeautifulSoup(res_books.text,\"html.parser\")\n",
    "\n",
    "#number of pages \n",
    "page_number = int(soup_book.find(\"li\",attrs={\"class\":\"current\"}).text.strip(\"\\n \").split(\"of \")[1])\n",
    "\n",
    "\n",
    "\n",
    "with open(\"Books_details.csv\", mode='w', encoding='utf-8') as fd:\n",
    "  fd.write(\"Full_Name,Name_in_books_page,Categoty,Price,Rating,Availability,Books_in_Stock,Link,Image\\n\") # header\n",
    "\n",
    "  for p in range(page_number):\n",
    "    page_url = \"https://books.toscrape.com/catalogue/page-\"+ str(p) + \".html\"\n",
    "\n",
    "    res_books = requests.get(page_url)\n",
    "    soup_book  = BeautifulSoup(res_books.text,\"html.parser\")\n",
    "\n",
    "    book_tag = soup_book.find_all(\"li\",attrs={\"class\":\"col-xs-6 col-sm-4 col-md-3 col-lg-3\"})\n",
    "    length_tag =  len(book_tag)\n",
    "\n",
    "    for i in range(len(book_tag)):\n",
    "\n",
    "      #Book Link\n",
    "      book_details_link = \"https://books.toscrape.com/catalogue/\" + book_tag[i].find(\"div\").find(\"a\").attrs['href']\n",
    "\n",
    "      #book image \n",
    "      book_image = book_tag[i].find(\"img\").attrs[\"src\"]\n",
    "      book_image = \"https://books.toscrape.com/\" + book_image[2:]\n",
    "\n",
    "      #book name\n",
    "      book_name = book_tag[i].find(\"h3\").find(\"a\").text\n",
    "\n",
    "      #book Price\n",
    "      book_price = book_tag[i].find(\"p\",attrs={\"class\":\"price_color\"}).text[2:]\n",
    "\n",
    "      #book Availability \n",
    "      book_availability = book_tag[i].find(\"p\",attrs={\"class\":\"instock availability\"}).text.strip(\"\\n \")\n",
    "\n",
    "      #book rating\n",
    "      book_rating = book_tag[i].find(\"p\").attrs[\"class\"][1]\n",
    "\n",
    "      #open book page \n",
    "      res_book_detail = requests.get(book_details_link)\n",
    "      soup_book_detail  = BeautifulSoup(res_book_detail.text,\"html.parser\")\n",
    "\n",
    "      #book full name from book page\n",
    "      book_full_name = soup_book_detail.find(\"h1\").text\n",
    "\n",
    "      #book Category\n",
    "      category_name=soup_book_detail.find(\"ul\",attrs={\"class\":\"breadcrumb\"}).find_all(\"li\")[2].text.strip(\"\\n\")\n",
    "\n",
    "      #number of bokks available in the stock\n",
    "      books_in_stock = soup_book_detail.find(\"p\",attrs={\"class\":\"instock availability\"}).text.strip(\"\\n \").split(\"(\")[1].split(\" \")[0]\n",
    "\n",
    "      fd.write(f\"\\\"{book_full_name}\\\",\\\"{book_name}\\\",\\\"{category_name}\\\",\\\"{book_price}\\\",\\\"{book_rating}\\\",\\\"{book_availability}\\\",\\\"{books_in_stock}\\\",\\\"{book_details_link}\\\",\\\"{book_image}\\\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4\n",
    "---------\n",
    "once you decided to do some exploring an want to know each country's (name, capital, population, Area(km^2) \n",
    "scrape the 250 countries (name, capital, population, area) and save them into a csv file\n",
    "use this site to scrape this data : https://www.scrapethissite.com/pages/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_countries = requests.get(\"https://www.scrapethissite.com/pages/simple/\")\n",
    "soup_countries  = BeautifulSoup(res_countries.text,\"html.parser\")\n",
    "countries_tag = soup_countries.find_all(\"div\",attrs={\"class\":\"col-md-4 country\"})\n",
    "num_countries = len(countries_tag)\n",
    "\n",
    "workbook = xlsxwriter.Workbook('Countries_write.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "col = 0\n",
    "#Excel Headder \n",
    "worksheet.write(0,col,\"Country\")\n",
    "worksheet.write(0,col+1,\"Capital\")\n",
    "worksheet.write(0,col+2,\"Population\")\n",
    "worksheet.write(0,col+3,\"Area\")\n",
    "\n",
    "with open(\"Countries.csv\",mode='w',encoding='utf-8') as fd:\n",
    "  fd.write(\"Country,Capital,Population,Area\\n\") # header\n",
    "  for i in range(num_countries):\n",
    "\n",
    "    col = 0\n",
    "    country_name = countries_tag[i].find(\"h3\").text.strip(\"\\n \")\n",
    "    country_capital = countries_tag[i].find(\"span\",attrs={\"class\":\"country-capital\"}).text\n",
    "    country_population = countries_tag[i].find(\"span\",attrs={\"class\":\"country-population\"}).text\n",
    "    country_area = countries_tag[i].find(\"span\",attrs={\"class\":\"country-area\"}).text\n",
    "    fd.write(f\"\\\"{country_name}\\\",\\\"{country_capital}\\\",\\\"{country_population}\\\",\\\"{country_area}\\\"\\n\")\n",
    "    worksheet.write(i+1,col,country_name)\n",
    "    worksheet.write(i+1,col+1,country_capital)\n",
    "    worksheet.write(i+1,col+2,country_population)\n",
    "    worksheet.write(i+1,col+3,country_area)\n",
    "    \n",
    " \n",
    "workbook.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Countries.csv\")\n",
    " \n",
    "# saving xlsx file\n",
    "df_xlsx = pd.ExcelWriter('Countries.xlsx')\n",
    "df.to_excel(df_xlsx, index=False)\n",
    "df_xlsx.close()\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5\n",
    "---------\n",
    "scraping data about hockey teams:\n",
    "you have to scrap the first page table's data (Team name, year, wins, losses, win%, goals for(gf), golaagainst(ga)\n",
    "from this website: https://www.scrapethissite.com/pages/forms/?page_num=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hockey = requests.get(\"https://www.scrapethissite.com/pages/forms/?page_num=1\")\n",
    "soup_hockey  = BeautifulSoup(res_hockey.text,\"html.parser\")\n",
    "\n",
    "hockey_table = soup_hockey.find(\"table\",attrs={\"class\":\"table\"})\n",
    "hockey_data = {}\n",
    "#header ( dynamic header )\n",
    "table_header = []\n",
    "hockey_data[1]=[\"M\"]\n",
    "hockey_header = hockey_table.find(\"tr\").find_all(\"th\")\n",
    "with open(\"Hockey.csv\",mode='w',encoding='utf-8') as fd:\n",
    "   for i in range (len(hockey_header)):\n",
    "      table_header.append(hockey_header[i].text.strip(\"\\n \"))\n",
    "      fd.write(f\"\\\"{table_header[i]}\\\",\")\n",
    "\n",
    "   fd.write(\"\\n\")\n",
    "\n",
    "   hockey_num_pages = soup_hockey.find(\"ul\",attrs={\"class\":\"pagination\"}).find_all(\"li\")\n",
    "   hockey_num_pages = len(hockey_num_pages)\n",
    "   \n",
    "   for p in range (hockey_num_pages):\n",
    "      hockey_URL = \"https://www.scrapethissite.com/pages/forms/?page_num=\" + str(p)\n",
    "      res_hockey = requests.get(hockey_URL)\n",
    "      soup_hockey  = BeautifulSoup(res_hockey.text,\"html.parser\")\n",
    "      hockey_table = soup_hockey.find(\"table\",attrs={\"class\":\"table\"})\n",
    "\n",
    "      hockey_row = hockey_table.find_all(\"tr\")\n",
    "\n",
    "      for r in range(1,len(hockey_row)):\n",
    "         hockey_data[r] = [ hockey_row[r].find_all(\"td\")[0].text.strip(\"\\n \") ]\n",
    "         for col in range (1,len(hockey_header)):\n",
    "            hockey_data[r] = hockey_data[r] + [hockey_row[r].find_all(\"td\")[col].text.strip(\"\\n \")]\n",
    "   \n",
    "      for r in range (1,len(hockey_row)):\n",
    "         for c in range(len(hockey_header)):\n",
    "            fd.write(f\"\\\"{hockey_data[r][c]}\\\",\")\n",
    "         \n",
    "         fd.write(\"\\n\")\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 6\n",
    "---------\n",
    "Use this web service \n",
    "https://jsonplaceholder.typicode.com/users\n",
    "and make a program that get all users data including name, username, email, street, suite, city, zip code, geo_lat & geo_long and then save them in a CSV & Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_data_link = \"https://jsonplaceholder.typicode.com/users\"\n",
    "res_users = requests.get(users_data_link)\n",
    "res_users = res_users.json()\n",
    "with open(\"Users_Data.csv\", mode='w', encoding='utf-8') as fd:\n",
    "    #Header\n",
    "    fd.write(\"ID,Name,User_Name,Email,Street,Suite,City,Zipcode,Geo_Lat,Geo_Lng,Phone,Website,Company_Name,Company_CatchPhrase,Company_Bs\\n\")\n",
    "    for i in range(len(res_users)):\n",
    "        user_id = res_users[i][\"id\"]\n",
    "        user_name = res_users[i][\"name\"]\n",
    "        user_username = res_users[i][\"username\"]\n",
    "        user_email = res_users[i][\"email\"]\n",
    "        user_street = res_users[i][\"address\"][\"street\"]\n",
    "        user_suite = res_users[i][\"address\"][\"suite\"]\n",
    "        user_city = res_users[i][\"address\"][\"city\"]\n",
    "        user_zipcode = res_users[i][\"address\"][\"zipcode\"]\n",
    "        user_geo_lat = res_users[i][\"address\"][\"geo\"][\"lat\"]\n",
    "        user_geo_lng = res_users[i][\"address\"][\"geo\"][\"lng\"]\n",
    "        user_phone = res_users[i][\"phone\"]\n",
    "        user_website = res_users[i][\"website\"]\n",
    "        user_company_name = res_users[i][\"company\"][\"name\"]\n",
    "        user_company_catchPhrase = res_users[i][\"company\"][\"catchPhrase\"]\n",
    "        user_company_bs  = res_users[i][\"company\"][\"bs\"]\n",
    "      \n",
    "        fd.write(f\"\\\"{user_id}\\\",\\\"{user_name}\\\",\\\"{user_username}\\\",\\\"{user_email}\\\",\\\"{user_street}\\\",\\\"{user_suite}\\\",\\\"{user_city}\\\",\\\"{user_zipcode}\\\",\\\"{user_geo_lat}\\\",\\\"{user_geo_lng}\\\",\\\"{user_phone}\\\",\\\"{user_website}\\\",\\\"{user_company_name}\\\",\\\"{user_company_catchPhrase}\\\",\\\"{user_company_bs}\\\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>User_Name</th>\n",
       "      <th>Email</th>\n",
       "      <th>Street</th>\n",
       "      <th>Suite</th>\n",
       "      <th>City</th>\n",
       "      <th>Zipcode</th>\n",
       "      <th>Geo_Lat</th>\n",
       "      <th>Geo_Lng</th>\n",
       "      <th>Phone</th>\n",
       "      <th>Website</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Company_CatchPhrase</th>\n",
       "      <th>Company_Bs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Leanne Graham</td>\n",
       "      <td>Bret</td>\n",
       "      <td>Sincere@april.biz</td>\n",
       "      <td>Kulas Light</td>\n",
       "      <td>Apt. 556</td>\n",
       "      <td>Gwenborough</td>\n",
       "      <td>92998-3874</td>\n",
       "      <td>-37.3159</td>\n",
       "      <td>81.1496</td>\n",
       "      <td>1-770-736-8031 x56442</td>\n",
       "      <td>hildegard.org</td>\n",
       "      <td>Romaguera-Crona</td>\n",
       "      <td>Multi-layered client-server neural-net</td>\n",
       "      <td>harness real-time e-markets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Ervin Howell</td>\n",
       "      <td>Antonette</td>\n",
       "      <td>Shanna@melissa.tv</td>\n",
       "      <td>Victor Plains</td>\n",
       "      <td>Suite 879</td>\n",
       "      <td>Wisokyburgh</td>\n",
       "      <td>90566-7771</td>\n",
       "      <td>-43.9509</td>\n",
       "      <td>-34.4618</td>\n",
       "      <td>010-692-6593 x09125</td>\n",
       "      <td>anastasia.net</td>\n",
       "      <td>Deckow-Crist</td>\n",
       "      <td>Proactive didactic contingency</td>\n",
       "      <td>synergize scalable supply-chains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Clementine Bauch</td>\n",
       "      <td>Samantha</td>\n",
       "      <td>Nathan@yesenia.net</td>\n",
       "      <td>Douglas Extension</td>\n",
       "      <td>Suite 847</td>\n",
       "      <td>McKenziehaven</td>\n",
       "      <td>59590-4157</td>\n",
       "      <td>-68.6102</td>\n",
       "      <td>-47.0653</td>\n",
       "      <td>1-463-123-4447</td>\n",
       "      <td>ramiro.info</td>\n",
       "      <td>Romaguera-Jacobson</td>\n",
       "      <td>Face to face bifurcated interface</td>\n",
       "      <td>e-enable strategic applications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Patricia Lebsack</td>\n",
       "      <td>Karianne</td>\n",
       "      <td>Julianne.OConner@kory.org</td>\n",
       "      <td>Hoeger Mall</td>\n",
       "      <td>Apt. 692</td>\n",
       "      <td>South Elvis</td>\n",
       "      <td>53919-4257</td>\n",
       "      <td>29.4572</td>\n",
       "      <td>-164.2990</td>\n",
       "      <td>493-170-9623 x156</td>\n",
       "      <td>kale.biz</td>\n",
       "      <td>Robel-Corkery</td>\n",
       "      <td>Multi-tiered zero tolerance productivity</td>\n",
       "      <td>transition cutting-edge web services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Chelsey Dietrich</td>\n",
       "      <td>Kamren</td>\n",
       "      <td>Lucio_Hettinger@annie.ca</td>\n",
       "      <td>Skiles Walks</td>\n",
       "      <td>Suite 351</td>\n",
       "      <td>Roscoeview</td>\n",
       "      <td>33263</td>\n",
       "      <td>-31.8129</td>\n",
       "      <td>62.5342</td>\n",
       "      <td>(254)954-1289</td>\n",
       "      <td>demarco.info</td>\n",
       "      <td>Keebler LLC</td>\n",
       "      <td>User-centric fault-tolerant solution</td>\n",
       "      <td>revolutionize end-to-end systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Mrs. Dennis Schulist</td>\n",
       "      <td>Leopoldo_Corkery</td>\n",
       "      <td>Karley_Dach@jasper.info</td>\n",
       "      <td>Norberto Crossing</td>\n",
       "      <td>Apt. 950</td>\n",
       "      <td>South Christy</td>\n",
       "      <td>23505-1337</td>\n",
       "      <td>-71.4197</td>\n",
       "      <td>71.7478</td>\n",
       "      <td>1-477-935-8478 x6430</td>\n",
       "      <td>ola.org</td>\n",
       "      <td>Considine-Lockman</td>\n",
       "      <td>Synchronised bottom-line interface</td>\n",
       "      <td>e-enable innovative applications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Kurtis Weissnat</td>\n",
       "      <td>Elwyn.Skiles</td>\n",
       "      <td>Telly.Hoeger@billy.biz</td>\n",
       "      <td>Rex Trail</td>\n",
       "      <td>Suite 280</td>\n",
       "      <td>Howemouth</td>\n",
       "      <td>58804-1099</td>\n",
       "      <td>24.8918</td>\n",
       "      <td>21.8984</td>\n",
       "      <td>210.067.6132</td>\n",
       "      <td>elvis.io</td>\n",
       "      <td>Johns Group</td>\n",
       "      <td>Configurable multimedia task-force</td>\n",
       "      <td>generate enterprise e-tailers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Nicholas Runolfsdottir V</td>\n",
       "      <td>Maxime_Nienow</td>\n",
       "      <td>Sherwood@rosamond.me</td>\n",
       "      <td>Ellsworth Summit</td>\n",
       "      <td>Suite 729</td>\n",
       "      <td>Aliyaview</td>\n",
       "      <td>45169</td>\n",
       "      <td>-14.3990</td>\n",
       "      <td>-120.7677</td>\n",
       "      <td>586.493.6943 x140</td>\n",
       "      <td>jacynthe.com</td>\n",
       "      <td>Abernathy Group</td>\n",
       "      <td>Implemented secondary concept</td>\n",
       "      <td>e-enable extensible e-tailers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Glenna Reichert</td>\n",
       "      <td>Delphine</td>\n",
       "      <td>Chaim_McDermott@dana.io</td>\n",
       "      <td>Dayna Park</td>\n",
       "      <td>Suite 449</td>\n",
       "      <td>Bartholomebury</td>\n",
       "      <td>76495-3109</td>\n",
       "      <td>24.6463</td>\n",
       "      <td>-168.8889</td>\n",
       "      <td>(775)976-6794 x41206</td>\n",
       "      <td>conrad.com</td>\n",
       "      <td>Yost and Sons</td>\n",
       "      <td>Switchable contextually-based project</td>\n",
       "      <td>aggregate real-time technologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Clementina DuBuque</td>\n",
       "      <td>Moriah.Stanton</td>\n",
       "      <td>Rey.Padberg@karina.biz</td>\n",
       "      <td>Kattie Turnpike</td>\n",
       "      <td>Suite 198</td>\n",
       "      <td>Lebsackbury</td>\n",
       "      <td>31428-2261</td>\n",
       "      <td>-38.2386</td>\n",
       "      <td>57.2232</td>\n",
       "      <td>024-648-3804</td>\n",
       "      <td>ambrose.net</td>\n",
       "      <td>Hoeger LLC</td>\n",
       "      <td>Centralized empowering task-force</td>\n",
       "      <td>target end-to-end models</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                      Name         User_Name                      Email  \\\n",
       "0   1             Leanne Graham              Bret          Sincere@april.biz   \n",
       "1   2              Ervin Howell         Antonette          Shanna@melissa.tv   \n",
       "2   3          Clementine Bauch          Samantha         Nathan@yesenia.net   \n",
       "3   4          Patricia Lebsack          Karianne  Julianne.OConner@kory.org   \n",
       "4   5          Chelsey Dietrich            Kamren   Lucio_Hettinger@annie.ca   \n",
       "5   6      Mrs. Dennis Schulist  Leopoldo_Corkery    Karley_Dach@jasper.info   \n",
       "6   7           Kurtis Weissnat      Elwyn.Skiles     Telly.Hoeger@billy.biz   \n",
       "7   8  Nicholas Runolfsdottir V     Maxime_Nienow       Sherwood@rosamond.me   \n",
       "8   9           Glenna Reichert          Delphine    Chaim_McDermott@dana.io   \n",
       "9  10        Clementina DuBuque    Moriah.Stanton     Rey.Padberg@karina.biz   \n",
       "\n",
       "              Street      Suite            City     Zipcode  Geo_Lat  \\\n",
       "0        Kulas Light   Apt. 556     Gwenborough  92998-3874 -37.3159   \n",
       "1      Victor Plains  Suite 879     Wisokyburgh  90566-7771 -43.9509   \n",
       "2  Douglas Extension  Suite 847   McKenziehaven  59590-4157 -68.6102   \n",
       "3        Hoeger Mall   Apt. 692     South Elvis  53919-4257  29.4572   \n",
       "4       Skiles Walks  Suite 351      Roscoeview       33263 -31.8129   \n",
       "5  Norberto Crossing   Apt. 950   South Christy  23505-1337 -71.4197   \n",
       "6          Rex Trail  Suite 280       Howemouth  58804-1099  24.8918   \n",
       "7   Ellsworth Summit  Suite 729       Aliyaview       45169 -14.3990   \n",
       "8         Dayna Park  Suite 449  Bartholomebury  76495-3109  24.6463   \n",
       "9    Kattie Turnpike  Suite 198     Lebsackbury  31428-2261 -38.2386   \n",
       "\n",
       "    Geo_Lng                  Phone        Website        Company_Name  \\\n",
       "0   81.1496  1-770-736-8031 x56442  hildegard.org     Romaguera-Crona   \n",
       "1  -34.4618    010-692-6593 x09125  anastasia.net        Deckow-Crist   \n",
       "2  -47.0653         1-463-123-4447    ramiro.info  Romaguera-Jacobson   \n",
       "3 -164.2990      493-170-9623 x156       kale.biz       Robel-Corkery   \n",
       "4   62.5342          (254)954-1289   demarco.info         Keebler LLC   \n",
       "5   71.7478   1-477-935-8478 x6430        ola.org   Considine-Lockman   \n",
       "6   21.8984           210.067.6132       elvis.io         Johns Group   \n",
       "7 -120.7677      586.493.6943 x140   jacynthe.com     Abernathy Group   \n",
       "8 -168.8889   (775)976-6794 x41206     conrad.com       Yost and Sons   \n",
       "9   57.2232           024-648-3804    ambrose.net          Hoeger LLC   \n",
       "\n",
       "                        Company_CatchPhrase  \\\n",
       "0    Multi-layered client-server neural-net   \n",
       "1            Proactive didactic contingency   \n",
       "2         Face to face bifurcated interface   \n",
       "3  Multi-tiered zero tolerance productivity   \n",
       "4      User-centric fault-tolerant solution   \n",
       "5        Synchronised bottom-line interface   \n",
       "6        Configurable multimedia task-force   \n",
       "7             Implemented secondary concept   \n",
       "8     Switchable contextually-based project   \n",
       "9         Centralized empowering task-force   \n",
       "\n",
       "                             Company_Bs  \n",
       "0           harness real-time e-markets  \n",
       "1      synergize scalable supply-chains  \n",
       "2       e-enable strategic applications  \n",
       "3  transition cutting-edge web services  \n",
       "4      revolutionize end-to-end systems  \n",
       "5      e-enable innovative applications  \n",
       "6         generate enterprise e-tailers  \n",
       "7         e-enable extensible e-tailers  \n",
       "8      aggregate real-time technologies  \n",
       "9              target end-to-end models  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Users_Data.csv\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
